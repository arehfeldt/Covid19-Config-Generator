{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arehfeldt/Covid19-Config-Generator/blob/main/Aaron_Rehfeldt_CSE5522_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTPwb8ny-hOU"
      },
      "source": [
        "**CSE 5522 Hands-on #1: Linear Regression**\n",
        "\n",
        "The goals of today's exercise are to familarize you with:\n",
        "\n",
        "\n",
        "*   Gradient descent\n",
        "*   Multivariate regression\n",
        "*   Data exploration\n",
        "\n",
        "**END OF CLASS GOAL:** Submit a link to your notebook (Share > Get Sharable Link) in Carmen so I can see how far you got.  This should be submitted in a group assignment page on Carmen.\n",
        "\n",
        "**Part 0: Initial setup**\n",
        "\n",
        "**0.0:** If none of your team members are familar with python this will be difficult to accomplish - you may want to split up and join different groups.\n",
        "\n",
        "**0.1:** Make a copy of this page in your google drive so that you can edit it. Share the copied page with your teammates. At the end of class, share a URL and submit (so I can see how far you got).  This will count as the participation grade for all members.\n",
        "\n",
        "**0.2:** While not completely necessary for this assignment, you may want to familiarize yourself with the following packages: [numpy](https://numpy.org), [scikit-learn](https://scikit-learn.org), [pandas](https://pandas.pydata.org), [matplotlib](https://matplotlib.org).\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2T5S8IQTct"
      },
      "source": [
        "**Part 1: Univariate Linear Regression & Gradient Descent**\n",
        "\n",
        "In class, we discussed how minimizing the Mean Squared Error criterion can lead to one solution of \"best fit\" for linear regression.  We looked at a dataset that related housing price and square footage in Columbus.  Your first task will be to replicate the regression algorithm that was demonstrated in class. Some of the more straightforward tasks will be given to you in code; some will need to be filled in.\n",
        "\n",
        "**1.0:** Set up the environment (you can click on the play button below to import the appropriate modules)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6fYsm5f-UbI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-MaOhV5UUvD"
      },
      "source": [
        "**1.1** Read the data from GitHub into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HskImt85UhU-"
      },
      "source": [
        "ColumbusHousingUrl='https://github.com/efosler/cse5522data/raw/master/columbus_price_vs_sf.csv'\n",
        "colhouse_dataframe=pd.read_csv(ColumbusHousingUrl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvgt0PYLVHYr"
      },
      "source": [
        "**1.2** Print out the top of the dataframe to make sure that the data loaded correctly.  It should be a data table with two columns (price, sf), and 5 rows, with the first row 220000,2240."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScERznWSVBK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fca4bfed-9b4c-4d3f-b2ff-6cb36156a787"
      },
      "source": [
        "colhouse_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>sf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>220000</td>\n",
              "      <td>2240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>171900</td>\n",
              "      <td>1720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>201500</td>\n",
              "      <td>2786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>140000</td>\n",
              "      <td>1040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>159000</td>\n",
              "      <td>2038</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    price    sf\n",
              "0  220000  2240\n",
              "1  171900  1720\n",
              "2  201500  2786\n",
              "3  140000  1040\n",
              "4  159000  2038"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AssaBgdR0K"
      },
      "source": [
        "**1.3:** At this point you may want to select columns from the dataframe; this is optional but may make debugging a bit easier as dataframes are not really arrays but are iterables.  Personally I like to use numpy arrays but you can choose to use standard python arrays if you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Q7tGhlVcOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2b77f5-3d8f-4d2e-fcb1-e4cfd02988e2"
      },
      "source": [
        "sf=np.array(colhouse_dataframe['sf'],dtype='float').reshape(-1,1)\n",
        "price=np.array(colhouse_dataframe['price'],dtype='float').reshape(-1,1)\n",
        "sf.shape\n",
        "price.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(222, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmXPr6qeSQo"
      },
      "source": [
        "**1.4:** Now the real work begins. Write a function that, given the dataset (sf,price) and a current set of weights, returns the gradient of the mean squared error.  You may wish to refer to the class notes on the MSE gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu3YKPlzeFLb"
      },
      "source": [
        "# computeMSEBatchGradient:\n",
        "#   weights - vector of weights (univariate linear = 2 weights)\n",
        "#   features - vector (or matrix) of feature values\n",
        "#   targets - vector of target values, same length as features\n",
        "#\n",
        "#   returns average gradient over the batch of features\n",
        "def computeMSEBatchGradient(weights,features,targets):\n",
        "\n",
        "  # insert calculation of gradient here\n",
        "  gradient = [0, 0]\n",
        "  for i in range(len(features)):\n",
        "    gradient[0] += targets[i] - (weights[1] * features[i] + weights[0])\n",
        "    gradient[1] += features[i] * (targets[i] - (weights[1] * features[i] + weights[0]))\n",
        "  gradient[0] *= -2/len(features)\n",
        "  gradient[1] *= -2/len(features)\n",
        "  return gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7F8osLy3ia"
      },
      "source": [
        "**1.5:** Next, write the function that calculates a weight update based on the gradient and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_J3BdyCfCVL"
      },
      "source": [
        "# computeGradientDescentWeightUpdate:\n",
        "#   weights - vector initial weights\n",
        "#   features - vector/matrix of features\n",
        "#   targets - vector of target values\n",
        "#   learning_rate - step length for update\n",
        "def computeGradientDescentWeightUpdate(weights, features, targets, learning_rate):\n",
        "  # insert calculation of weight update here\n",
        "  gradient = computeMSEBatchGradient(weights, features, targets)\n",
        "  weights[0] = weights[0] - learning_rate * gradient[0]\n",
        "  weights[1] = weights[1] - learning_rate * gradient[1]\n",
        "  return weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJ97nqVz8CW"
      },
      "source": [
        "**1.6:** Now write the outer wrapper that initializes the weights and learning rate and runs the gradient update in a loop.  You can choose to run for a fixed number of iterations, or test for convergence by seeing if the weights change more than some small amount.  Print out the weights at the end.   *Note: if your code runs for more than a minute then you probably have a diverging set of weights.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNGxLT9qzOXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f6350e-919c-4a8d-9728-d6dcb2372c7d"
      },
      "source": [
        "# insert wrapper code here\n",
        "weights = [0, 0]\n",
        "learning_rate = 0.001\n",
        "for i in range(1000):\n",
        "  weights = computeGradientDescentWeightUpdate(weights, sf, price, learning_rate)\n",
        "weights\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in multiply\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in subtract\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([nan]), array([nan])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InZwM9Fv04R5"
      },
      "source": [
        "**1.7:** Compare your results against a known linear regression algorithm from *scikit-learn*.  Do you get the same values for slope and intercept?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VauBe0gZ029E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a7705b-4e55-4d56-d6e8-3f5fda128d7a"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_model=LinearRegression()\n",
        "lin_model.fit(sf,price)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR81RBzh2AW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27385292-85bc-49ca-e61b-f0a7c8720b6a"
      },
      "source": [
        "# print slope\n",
        "lin_model.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[110.08078024]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q4JIf7w2MwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a12162-8147-4dfa-a8d8-44ef0fc618d9"
      },
      "source": [
        "# print intercept\n",
        "lin_model.intercept_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-39486.50513007])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSuce_0g3Uqx"
      },
      "source": [
        "**1.8:** At this point, go back to 1.6 and try tuning the learning rate.  Can you notice any differences in the algorithm's performance?  If your algorithm didn't converge previously, you may want to adjust the scale of your data (maybe make them fall in roughly the same range), or have multiple learning rates.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*Hint: you may want to print the weights every 1000 updates or so to see how the weights change over time.  If you want to get really tricky you can try to reimplment the movie we showed in class, but probably beyond the scope of this exercise.*\n",
        "\n",
        "---\n",
        "\n",
        "One trick that ML practitioners often do is to *normalize* the data by subtracting the mean and dividing by the standard deviation; this means that you can usually use a learning rate in roughly the same ballpark each time.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYMPnwak4xhX"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "**Part 2: Multivariate regression**\n",
        "\n",
        "In this part of the hands-on class, you'll explore multivariate regression.  We'll use the *scikit-learn* linear regression tool (although you're welcome to extend your code above if you wish).  The primary reason for this is that the sklearn routines will handle some of the normalization issues for you.\n",
        "\n",
        "Some parts of this tutorial were based on [Argawal, Linear Regression on Boston Housing Dataset](https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155).\n",
        "\n",
        "**2.0:** We'll be looking at the Boston Housing dataset which is built in to sklearn, but has more variables. Load the dataset in:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTzAMOTl4v_f"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston_dataset=load_boston()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csgSBR5w6FKq"
      },
      "source": [
        "**2.1** You can see a description of the dataset by printing the dataset's DESCR attribute (set by the sklearn.dataset package)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez-qWyVI6CAL"
      },
      "source": [
        "print(boston_dataset.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1czgqWq7N5d"
      },
      "source": [
        "**2.2:** We can convert this data into a pandas dataframe and then look at the first few rows in tabular form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phA72blh6d_N"
      },
      "source": [
        "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "boston['MEDV']=boston_dataset.target\n",
        "boston.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIynx6z4AA3I"
      },
      "source": [
        "**2.3:** At this point, we want to figure out what variables are good predictors of the median value of a house in Boston.  We can find the relationship between single variables by looking at the correlation between variables.  \n",
        "\n",
        "---\n",
        "\n",
        "*Side note: the correlation between two vectors of variables x and y is*\n",
        "```\n",
        "1/(n-1) sum x*y\n",
        "```\n",
        "*assuming that x and y have zero mean and a standard gaussian distribution (which you can achieve by subtracting the mean and dividing by the standard deviation).  If x and y always share the same sign, then this value will be positive; if x and y are always opposite sign then the correlation is negative.*\n",
        "\n",
        "---\n",
        "\n",
        "The sklearn.datasets class has a built in correlation function, which we can use to compute the correlation between each pair of variables.  We can also use the seaborn package to plot a nice heatmap of these variables.\n",
        "\n",
        "**What's the most correlated variable with MEDV?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpPMqeZ37esY"
      },
      "source": [
        "import seaborn as sb\n",
        "\n",
        "correlation_matrix = boston.corr().round(2)\n",
        "plt.figure(figsize=(10,8))\n",
        "sb.heatmap(data=correlation_matrix, annot=True, center=0.0, cmap='coolwarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN-4LVKcM3Hp"
      },
      "source": [
        "Note that the most correlated variable is LSTAT (at -0.74) and not RM (0.70) - while the correlation is negative, it just means that there is an inverse relationship between LSTAT and MEDV, but LSTAT should be the single best predictor.\n",
        "\n",
        "**2.4:** Now, we kind of cheated there, since we figured out the correlation on the entire dataset.  A better practice is to randomly select training and test sets, fit the data on the training set, and then evaluate on the test set.  Let's see if LSTAT really is a better predictor than RM of MEDV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHsrFaJf8WvS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# first, set up dataframe for all variables\n",
        "boston_totaldata = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "boston_totaltarget = boston_dataset.target\n",
        "\n",
        "boston_traindata, boston_testdata, boston_traintarget, boston_testtarget = train_test_split(boston_totaldata,\n",
        "                                                                                           boston_totaltarget,\n",
        "                                                                                           test_size=0.2,\n",
        "                                                                                           random_state=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koimxdTnPJYr"
      },
      "source": [
        "boston_traindata.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8roAOT6Pd0f"
      },
      "source": [
        "boston_testdata.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF_qw_msPkkJ"
      },
      "source": [
        "# subselect LSTAT data, train model\n",
        "#   note the double square brackets - need 2-dimensional array to come out of selection\n",
        "lstat_traindata=boston_traindata[['LSTAT']]\n",
        "lstat_testdata=boston_testdata[['LSTAT']]\n",
        "lin_model = LinearRegression()\n",
        "lin_model.fit(lstat_traindata,boston_traintarget)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWDGQYlsQpqX"
      },
      "source": [
        "lstat_testpredict=lin_model.predict(lstat_testdata)\n",
        "rmse = (np.sqrt(mean_squared_error(boston_testtarget,lstat_testpredict)))\n",
        "r2 = r2_score(boston_testtarget,lstat_testpredict)\n",
        "print('Test RMSE = {}, Test R2 = {}'.format(rmse,r2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uxds-NwjrVZ"
      },
      "source": [
        "**2.4a:** R2 is the amount of variation explained by the model, and can range from 0 (no variance explained) to 1 (perfect explanation). Notice that R2 is much lower than the correlation coefficient.  Why? (There are 2 reasons.)\n",
        "\n",
        "Let's do it again for RM (this time, you calculate it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MpXx-55QsBG"
      },
      "source": [
        "# fill in code to calculate RMSE, R2 for RM variable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJ13awBkk3-"
      },
      "source": [
        "(Note: answers for why R2 is lower that correlation is that (a) it's roughly the square of correlation, and also (b) we calculated correlation on the entire training set; R2 was calculated on test only.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By0YSrH8k8CQ"
      },
      "source": [
        "**2.5:** At this point, you can start to look at how to predict from multiple variables.  What are the two best variables in predicting as measured by R2?\n",
        "\n",
        "Are the results better than predicting using all variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cHvWvcrnQ-I"
      },
      "source": [
        "# Rebuild your training set to look at two variable prediction."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7FHp8Qn2zOl"
      },
      "source": [
        "---\n",
        "---\n",
        "**Part 3: Predicting Gene Expression**\n",
        "\n",
        "There are ~20,000 genes in the human genome. Each one of them is transcribed to mRNA and then translated to proteins which carry on various tasks inside our body. We can measure the amount of 20,000 mRNA expressed in samples collected from different organs. This collection is called _gene expression profile_.\n",
        "\n",
        "Although our genome is the same across all cell types, the gene expression profile is different because each organ needs different proteins for its survival. One of the regulatory mechanisms which controls the expression level in each cell type is microRNA (miR). MicroRNAs are small molecules which attach to mRNAs and prevent them from translation to proteins and also degrade them.\n",
        "\n",
        "So if miR A targets mRNA B when A increases B decreases. Our goal is to predict mRNA levels (gene expression profile) using 21 miR features. Note that each of the 20,000 expression levels can be a response of regression with 21 features. To simplify, we have selected a few genes to predict their expression.\n",
        "\n",
        "This part of the exercise is deliberately more open-ended.\n",
        "\n",
        "**Instructions:** Load the data from GitHub (example code given).  Use the Boston Housing example to *randomly* break the data into an 80/20 split across the 8895 examples.  Pick a few random examples from the well expressed set and from the poorly expressed set and train a linear regression predictor for each mRNA.  Describe the differences you see across the two sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu-wHw6wFTYv"
      },
      "source": [
        "miR_URL='https://raw.githubusercontent.com/efosler/cse5522data/master/miRScore-x.csv'\n",
        "miR_df=pd.read_csv(miR_URL)\n",
        "# remove row names from the frist column, rows=samples, cols=miR features\n",
        "miR = np.array(miR_df)[:,1:-1]\n",
        "\n",
        "mRNA_well_URL='https://raw.githubusercontent.com/efosler/cse5522data/master/mRNA-y-well-explained.csv'\n",
        "mRNA_well_df=pd.read_csv(mRNA_well_URL)\n",
        "# remove label column, transpose with rows=samples, cols=mRNA\n",
        "mRNA_well = np.transpose(np.array(mRNA_well_df)[:,1:8896])\n",
        "\n",
        "mRNA_poor_URL='https://raw.githubusercontent.com/efosler/cse5522data/master/mRNA-y-poor-explained.csv'\n",
        "mRNA_poor_df=pd.read_csv(mRNA_poor_URL)\n",
        "# remove label column, transpose with rows=samples, cols=mRNA\n",
        "mRNA_poor = np.transpose(np.array(mRNA_poor_df)[:,1:8896])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFD0JpVzLTps"
      },
      "source": [
        "# Carry on from this point on your own, using the Boston Housing problem as a guide.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}